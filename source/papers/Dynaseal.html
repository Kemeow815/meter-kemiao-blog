<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynaseal - Secure LLM Agent Framework</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            padding: 0;
            color: #333;
        }
        header {
            background-color: #dad8d8;
            color: #fff;
            padding: 20px 0;
            text-align: center;
        }
        main {
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
        }
        h1, h2 {
            color: #333;
        }
        p {
            line-height: 1.6;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: relative;
            width: 100%;
            bottom: 0;
        }
        a {
            color: #007BFF;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>

    <header>
        <h1>Dynaseal Paper</h1>
        <p>Click the link below to download the Dynaseal paper:</p>
        <a href="https://studyinglover.com/papers/Dynaseal.pdf" target="_blank">Download Dynaseal Paper</a>
        
    </header>

    <main>
        <section>
            <h2>Why Dynaseal?</h2>
            <p>Imagine a future where LLM agents are ubiquitous on the edge devices. However, the current method of accessing large models through an easily accessible API-key could lead to potential abuse, such as excessive billing or server overload. Dynaseal addresses this issue by introducing a dynamic token system that limits the scope and duration of API access, ensuring secure and controlled communication between edge devices and large model providers.</p>
        </section>

        <section>
            <h2>Project Overview</h2>
            <p>Dynaseal is a server-side framework designed for future edge-based LLM agent models. It employs a dynamic token mechanism, similar to OSS, to restrict the models and parameters that can be accessed by edge agents, along with the token's lifespan. It supports direct communication between edge devices and large model providers, with a callback mechanism to notify the backend upon response completion.</p>
        </section>

        <section>
            <h2>System Design</h2>
            <p>The architecture consists of three main components: LLM Server, Backend, and Client.</p>
            <ol>
                <li><strong>Backend Initialization:</strong> Requests LLM API-key from the LLM Server.</li>
                <li><strong>Client Initialization:</strong> Requests authentication from the Backend, which returns a dynamic key specifying models, tokens, and other essential information.</li>
                <li><strong>Client Request:</strong> Uses the dynamic key to request models from the LLM Server.</li>
                <li><strong>LLM Server Response:</strong> Unpacks the dynamic key, verifies identity, and generates a response.</li>
                <li><strong>Callback Notification:</strong> The LLM Server notifies the Backend of the client's request and response.</li>
            </ol>
        </section>

        <section>
            <h2>Dynamic Key Design</h2>
            <p>The dynamic key is designed as a JWT token (`header.payload.secret`). The payload contains essential information such as the API-key, model, maximum tokens, expiration time, and event ID. The secret is encrypted using the backend's registered key, ensuring the dynamic key's validity.</p>
        </section>

        <section>
            <h2>Implementation</h2>
            <p>The project is divided into three main folders:</p>
            <ul>
                <li><strong>LLM Server:</strong> The large model backend, which includes authentication and response mechanisms for dynamic keys.</li>
                <li><strong>Backend:</strong> The business backend that authenticates clients and distributes dynamic keys.</li>
                <li><strong>Client:</strong> The edge device running the agent.</li>
            </ul>
        </section>

        <section>
            <h2>Getting Started</h2>
            <p>To start the project:</p>
            <ol>
                <li>Launch the LLM Server.</li>
                <li>Launch the Backend.</li>
                <li>Run the Client to verify successful invocation.</li>
            </ol>
        </section>

        <section>
            <h2>Future Work</h2>
            <p>Some features yet to be implemented include:</p>
            <ul>
                <li>Storing callback requests in the database.</li>
                <li>LLM Server and Backend registration.</li>
                <li>And more...</li>
            </ul>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Dynaseal. All rights reserved.</p>
        <p><a href="https://github.com/IntelliKernel/Dynaseal">Visit our GitHub repository</a></p>
    </footer>

</body>
</html>